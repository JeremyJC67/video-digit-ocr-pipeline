#!/usr/bin/env python3
"""
Post-processing script that visualizes Qwen OCR results on top of the source video.

Inputs:
    --video: original video file
    --csv:   results.csv generated by qwen_frame_reader.py (frame,timestamp,temp_c)
    --out_mp4: path to the annotated MP4

Optional:
    --roi: manual ROI override formatted as x,y,w,h
    --auto_from_first_valid: auto-detect ROI from the first frame that has a valid temp

This script never calls Qwen/Transformers again; it simply replays the frames,
highlights the digits region, overlays the temp text, and exports a new video.
"""

from __future__ import annotations

import argparse
import csv
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Sequence, Tuple

import cv2
import numpy as np
from tqdm import tqdm


@dataclass
class TempRow:
    frame: int
    timestamp: str
    temp_c: Optional[float]


def parse_roi(arg: Optional[str]) -> Optional[Tuple[int, int, int, int]]:
    if not arg:
        return None
    parts = [p.strip() for p in arg.split(",")]
    if len(parts) != 4:
        raise ValueError("--roi must be formatted as x,y,w,h")
    roi = tuple(int(v) for v in parts)
    x, y, w, h = roi
    if min(w, h) <= 0:
        raise ValueError("ROI width and height must be positive")
    return x, y, w, h


def read_csv(path: Path) -> List[TempRow]:
    rows: List[TempRow] = []
    with path.open("r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for line in reader:
            frame_str = line.get("frame")
            timestamp = line.get("timestamp") or ""
            temp_str = (line.get("temp_c") or "").strip()
            if frame_str is None:
                continue
            try:
                frame_idx = int(frame_str)
            except ValueError:
                continue
            temp_val: Optional[float]
            if temp_str == "":
                temp_val = None
            else:
                try:
                    temp_val = float(temp_str)
                except ValueError:
                    temp_val = None
            rows.append(TempRow(frame=frame_idx, timestamp=timestamp, temp_c=temp_val))
    rows.sort(key=lambda r: r.frame)
    return rows


def auto_detect_temp_roi(frame_bgr: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
    height, width = frame_bgr.shape[:2]
    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)

    lower_blue = np.array([90, 40, 40], dtype=np.uint8)
    upper_blue = np.array([140, 255, 255], dtype=np.uint8)
    blue_mask = cv2.inRange(hsv, lower_blue, upper_blue)
    kernel = np.ones((5, 5), np.uint8)
    blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_CLOSE, kernel, iterations=2)

    contours, _ = cv2.findContours(blue_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None

    frame_area = float(width * height)
    candidates = []
    for cnt in contours:
        area = cv2.contourArea(cnt)
        if area < 0.01 * frame_area:
            continue
        x, y, w, h = cv2.boundingRect(cnt)
        candidates.append((area, (x, y, w, h)))
    if not candidates:
        return None
    candidates.sort(reverse=True)
    panel_x, panel_y, panel_w, panel_h = candidates[0][1]

    panel_roi = hsv[panel_y : panel_y + panel_h, panel_x : panel_x + panel_w]
    if panel_roi.size == 0:
        return None
    lower_white = np.array([0, 0, 170], dtype=np.uint8)
    upper_white = np.array([179, 80, 255], dtype=np.uint8)
    white_mask = cv2.inRange(panel_roi, lower_white, upper_white)
    white_mask = cv2.morphologyEx(white_mask, cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8), iterations=2)

    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(white_mask, connectivity=8)
    band_candidates = []
    for idx in range(1, num_labels):
        area = stats[idx, cv2.CC_STAT_AREA]
        x = stats[idx, cv2.CC_STAT_LEFT]
        y = stats[idx, cv2.CC_STAT_TOP]
        w = stats[idx, cv2.CC_STAT_WIDTH]
        h = stats[idx, cv2.CC_STAT_HEIGHT]
        center_y = y + h / 2.0
        width_ratio = w / max(panel_w, 1)
        height_ratio = h / max(panel_h, 1)
        center_ratio = center_y / max(panel_h, 1)
        if width_ratio < 0.4:
            continue
        if not (0.1 <= height_ratio <= 0.4):
            continue
        if not (0.2 <= center_ratio <= 0.7):
            continue
        band_candidates.append((area, (x, y, w, h)))

    if not band_candidates:
        return None

    band_candidates.sort(reverse=True)
    band_x, band_y, band_w, band_h = band_candidates[0][1]
    pad = max(2, int(0.03 * panel_h))
    x1 = max(0, panel_x + band_x - pad)
    y1 = max(0, panel_y + band_y - pad)
    x2 = min(width, panel_x + band_x + band_w + pad)
    y2 = min(height, panel_y + band_y + band_h + pad)
    if x1 >= x2 or y1 >= y2:
        return None
    return x1, y1, x2 - x1, y2 - y1


def highlight_digits(frame_bgr: np.ndarray, roi: Optional[Tuple[int, int, int, int]], min_area: int = 25) -> np.ndarray:
    if frame_bgr is None or roi is None:
        return frame_bgr
    x, y, w, h = roi
    height, width = frame_bgr.shape[:2]
    x1 = max(0, min(width, x))
    y1 = max(0, min(height, y))
    x2 = max(0, min(width, x + w))
    y2 = max(0, min(height, y + h))
    if x1 >= x2 or y1 >= y2:
        return frame_bgr
    roi_slice = frame_bgr[y1:y2, x1:x2]
    if roi_slice.size == 0:
        return frame_bgr
    gray = cv2.cvtColor(roi_slice, cv2.COLOR_BGR2GRAY)
    _, mask = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)
    kernel = np.ones((3, 3), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)
    num_labels, _, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)
    boxes = []
    roi_h = y2 - y1
    for idx in range(1, num_labels):
        area = stats[idx, cv2.CC_STAT_AREA]
        if area < min_area:
            continue
        left = stats[idx, cv2.CC_STAT_LEFT]
        top = stats[idx, cv2.CC_STAT_TOP]
        width_local = stats[idx, cv2.CC_STAT_WIDTH]
        height_local = stats[idx, cv2.CC_STAT_HEIGHT]
        if height_local < 0.1 * roi_h:
            continue
        pad = 2
        bx = max(0, x1 + left - pad)
        by = max(0, y1 + top - pad)
        bw = width_local + 2 * pad
        bh = height_local + 2 * pad
        bw = min(bw, width - bx)
        bh = min(bh, height - by)
        boxes.append((bx, by, bw, bh))
    for bx, by, bw, bh in sorted(boxes, key=lambda b: b[0]):
        cv2.rectangle(frame_bgr, (bx, by), (bx + bw, by + bh), (0, 255, 0), 2)
    return frame_bgr


def overlay_temp_text(frame_bgr: np.ndarray, text: str, origin: Tuple[int, int] = (20, 40)) -> None:
    cv2.putText(
        frame_bgr,
        text,
        origin,
        cv2.FONT_HERSHEY_SIMPLEX,
        1.0,
        (0, 255, 0),
        thickness=2,
        lineType=cv2.LINE_AA,
    )


def annotate_video(
    video_path: Path,
    csv_path: Path,
    out_path: Path,
    roi: Optional[Tuple[int, int, int, int]],
    auto_from_first_valid: bool,
) -> None:
    rows = read_csv(csv_path)
    if not rows:
        raise RuntimeError(f"No valid rows found in {csv_path}")

    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        raise RuntimeError(f"Unable to open video: {video_path}")

    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)
    if width <= 0 or height <= 0:
        raise RuntimeError("Failed to read video dimensions")

    out_path.parent.mkdir(parents=True, exist_ok=True)
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    writer = cv2.VideoWriter(str(out_path), fourcc, fps, (width, height))

    auto_roi = roi
    auto_done = roi is not None

    try:
        for row in tqdm(rows, desc="Annotating frames"):
            cap.set(cv2.CAP_PROP_POS_FRAMES, row.frame)
            ok, frame = cap.read()
            if not ok or frame is None:
                continue

            if (
                not auto_done
                and auto_from_first_valid
                and row.temp_c is not None
            ):
                detected = auto_detect_temp_roi(frame)
                if detected:
                    auto_roi = detected
                    auto_done = True
                    x, y, w, h = detected
                    print(f"Auto-detected ROI: x={x}, y={y}, w={w}, h={h}")
                else:
                    print("Auto-detect ROI failed; continuing without ROI")
                    auto_done = True  # avoid repeated attempts

            annotated = frame.copy()
            if auto_roi is not None:
                highlight_digits(annotated, auto_roi)

            if row.temp_c is None:
                temp_text = "temp_c = N/A"
            else:
                temp_text = f"temp_c = {row.temp_c:.1f} Â°C"
            overlay_temp_text(annotated, temp_text, origin=(20, 40))
            overlay_temp_text(annotated, f"frame {row.frame} | {row.timestamp}", origin=(20, 80))

            writer.write(annotated)
    finally:
        cap.release()
        writer.release()


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Annotate video frames using existing Qwen CSV results.")
    parser.add_argument("--video", required=True, help="Path to the source video file.")
    parser.add_argument("--csv", required=True, help="Path to qwen_frame_reader results.csv.")
    parser.add_argument("--out_mp4", required=True, help="Path to the annotated MP4 output.")
    parser.add_argument("--roi", type=str, default=None, help="Manual ROI override formatted as x,y,w,h.")
    parser.add_argument(
        "--auto_from_first_valid",
        dest="auto_from_first_valid",
        action="store_true",
        default=True,
        help="Auto-detect ROI from the first valid temp frame (default: enabled).",
    )
    parser.add_argument(
        "--no-auto_from_first_valid",
        dest="auto_from_first_valid",
        action="store_false",
        help="Disable automatic ROI detection.",
    )
    return parser


def main() -> None:
    parser = build_arg_parser()
    args = parser.parse_args()

    video_path = Path(args.video)
    csv_path = Path(args.csv)
    out_path = Path(args.out_mp4)
    roi = parse_roi(args.roi)
    auto_flag = args.auto_from_first_valid and roi is None

    annotate_video(
        video_path=video_path,
        csv_path=csv_path,
        out_path=out_path,
        roi=roi,
        auto_from_first_valid=auto_flag,
    )
    print(f"Wrote annotated video to {out_path}")


if __name__ == "__main__":
    main()
